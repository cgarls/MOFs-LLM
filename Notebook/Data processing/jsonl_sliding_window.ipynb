{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gatsby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gatsby\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script provides utilities for processing large JSONL files:\n",
    "1. split_jsonl(): Splits a large JSONL file into smaller chunks with specified size\n",
    "2. slide_window(): Implements sliding window approach to split long text content while maintaining context\n",
    "3. process_jsonl(): Processes JSONL files by splitting long content using sliding window\n",
    "\n",
    "The main purpose is to handle large text data and prepare it for further processing or model training\n",
    "by ensuring each chunk stays within token limits while preserving context through overlapping.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "# Ensure nltk punkt tokenizer is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "def split_jsonl(input_file, output_prefix, chunk_size):\n",
    "    with open(input_file, 'r', encoding='utf-8') as in_file:\n",
    "        data = []\n",
    "        file_number = 1\n",
    "        for line in in_file:\n",
    "            data.append(json.loads(line))\n",
    "            if len(data) == chunk_size:\n",
    "                with open(f'{output_prefix}_{file_number}.jsonl', 'w', encoding='utf-8') as out_file:\n",
    "                    for item in data:\n",
    "                        out_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                data = []\n",
    "                file_number += 1\n",
    "        if data:\n",
    "            with open(f'{output_prefix}_{file_number}.jsonl', 'w', encoding='utf-8') as out_file:\n",
    "                for item in data:\n",
    "                    out_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def slide_window(content, max_tokens=7000, overlap_tokens=200):\n",
    "    # Split content into sentences using nltk tokenizer\n",
    "    sentences = nltk.tokenize.sent_tokenize(content)\n",
    "    current_chunk = []\n",
    "    chunks = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(nltk.tokenize.word_tokenize(sentence))\n",
    "        if current_length + sentence_length > max_tokens and current_chunk:\n",
    "            # Save current chunk when max token count is reached\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            # Backtrack to find overlap section containing ~200-300 tokens\n",
    "            overlap_text = ''\n",
    "            overlap_length = 0\n",
    "            for sent in reversed(current_chunk):\n",
    "                sent_length = len(nltk.tokenize.word_tokenize(sent))\n",
    "                if overlap_length + sent_length > overlap_tokens:\n",
    "                    break\n",
    "                overlap_text = sent + ' ' + overlap_text\n",
    "                overlap_length += sent_length\n",
    "            # Keep overlap section as start of next chunk\n",
    "            current_chunk = nltk.tokenize.sent_tokenize(overlap_text)\n",
    "            current_length = overlap_length\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length\n",
    "\n",
    "    # Save the final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def process_jsonl(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            content = data['content']\n",
    "            processed_contents = slide_window(content)\n",
    "            for content_chunk in processed_contents:\n",
    "                json_record = json.dumps({'content': content_chunk}, ensure_ascii=False)\n",
    "                outfile.write(json_record + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_file = './data/input.jsonl'  # Input JSONL file path\n",
    "output_prefix = './data/output'  # Output prefix for split files\n",
    "chunk_size = 200\n",
    "split_jsonl(input_file, output_prefix, chunk_size)\n",
    "\n",
    "# Process file with sliding window\n",
    "input_jsonl = './data/source.jsonl'\n",
    "output_jsonl = './data/processed.jsonl'\n",
    "process_jsonl(input_jsonl, output_jsonl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
