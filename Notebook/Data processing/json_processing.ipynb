{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the longest element is: 40923\n",
      "Modified strings have been written in JSON format to 'D:\\项目记录\\LLM数据\\mat_jsonl\\3.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "# This script processes a JSON Lines file to extract specific text data related to MOFs (Metal-Organic Frameworks), \n",
    "# finds the longest text entry, and formats the extracted data into a new JSON Lines file.\n",
    "\n",
    "import json\n",
    "mat_data = []\n",
    "\n",
    "def filter_json_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if 'text' in data and (\"(MOF)\" in data['text'] or \"(MOFs)\" in data['text'] or \"MOF-\" in data['text'] or \"aterial\" in data['text']):\n",
    "                mat_data.append(data['text'])\n",
    "\n",
    "# Example call\n",
    "file_path = r'D:\\项目记录\\LLM数据\\mat\\train-00002-of-00020.json\\s2ag.train.json'\n",
    "filter_json_data(file_path)\n",
    "\n",
    "max_length = 0\n",
    "max_index = 0\n",
    "for idx, text in enumerate(mat_data):\n",
    "    if len(text) > max_length:\n",
    "        max_length = len(text)\n",
    "        max_index = idx\n",
    "\n",
    "print(\"The index of the longest element is:\", max_index)\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "a = nlp(mat_data[max_index])\n",
    "len(a)\n",
    "\n",
    "# Add '{\"content\": \"' prefix and '\", \"}' suffix to each string, preserving '\\n'\n",
    "modified_strings = ['{\"content\": \"' + text.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"').replace('</p>', ' ').replace('<p>', ' ').replace('\\n', '\\\\n') + '\"}' for text in mat_data]\n",
    "\n",
    "# Write each modified string as a separate line in a JSON file\n",
    "output_file_path = r'D:\\项目记录\\LLM数据\\mat_jsonl\\3.jsonl'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    for line in modified_strings:\n",
    "        file.write(f\"{line}\\n\")\n",
    "\n",
    "print(f\"Modified strings have been written in JSON format to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script validates a JSON Lines file by checking each line for proper JSON syntax. \n",
    "# If any lines contain errors, they are deleted, and a summary of the process is printed.\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def validate_jsonl(file_path):\n",
    "    lines_to_delete = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(file, start=1):\n",
    "                original_line = line.strip()\n",
    "                try:\n",
    "                    json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error in line {line_number}: {e}\")\n",
    "                    print(f\"Error line content: {original_line}\")\n",
    "                    lines_to_delete.append(line_number)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return False\n",
    "\n",
    "    # Delete erroneous lines\n",
    "    if lines_to_delete:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(lines, start=1):\n",
    "                if line_number not in lines_to_delete:\n",
    "                    file.write(line)\n",
    "\n",
    "        print(f\"Deleted {len(lines_to_delete)} lines with errors.\")\n",
    "    \n",
    "    print(f\"JSON Lines file '{file_path}' syntax is valid after processing!\")\n",
    "    return True\n",
    "\n",
    "# Specify your JSON Lines file path\n",
    "jsonl_file_path = \"path/to/your/jsonl_file.jsonl\"\n",
    "validate_jsonl(jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script splits a JSON Lines file into smaller chunks of a specified size. \n",
    "# Each chunk is saved as a separate JSON Lines file with a given prefix.\n",
    "\n",
    "import json\n",
    "\n",
    "def split_jsonl(input_file, output_prefix, chunk_size):\n",
    "    with open(input_file, 'r', encoding='utf-8') as in_file:\n",
    "        data = []\n",
    "        file_number = 1\n",
    "        for line in in_file:\n",
    "            data.append(json.loads(line))\n",
    "            if len(data) == chunk_size:\n",
    "                with open(f'{output_prefix}_{file_number}.jsonl', 'w', encoding='utf-8') as out_file:\n",
    "                    for item in data:\n",
    "                        out_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                data = []\n",
    "                file_number += 1\n",
    "        if data:\n",
    "            with open(f'{output_prefix}_{file_number}.jsonl', 'w', encoding='utf-8') as out_file:\n",
    "                for item in data:\n",
    "                    out_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_file = 'path/to/your/input_file.jsonl'\n",
    "output_prefix = 'output'\n",
    "chunk_size = 200\n",
    "split_jsonl(input_file, output_prefix, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Lines file 'D:\\项目记录\\LLM数据\\without_doi_2000MOFs.jsonl' syntax is valid after processing!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This script validates a JSON Lines file by checking each line for valid JSON syntax. \n",
    "# It removes any lines that contain errors and reports the number of deleted lines.\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def validate_jsonl(file_path):\n",
    "    lines_to_delete = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(file, start=1):\n",
    "                original_line = line.strip()\n",
    "                try:\n",
    "                    json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error in line {line_number}: {e}\")\n",
    "                    print(f\"Error line content: {original_line}\")\n",
    "                    lines_to_delete.append(line_number)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return False\n",
    "\n",
    "    # Remove erroneous lines\n",
    "    if lines_to_delete:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(lines, start=1):\n",
    "                if line_number not in lines_to_delete:\n",
    "                    file.write(line)\n",
    "\n",
    "        print(f\"Deleted {len(lines_to_delete)} lines with errors.\")\n",
    "    \n",
    "    print(f\"JSON Lines file '{file_path}' syntax is valid after processing!\")\n",
    "    return True\n",
    "\n",
    "# Specify your JSON Lines file path\n",
    "jsonl_file_path = \"path/to/your/jsonl_file.jsonl\"\n",
    "validate_jsonl(jsonl_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
